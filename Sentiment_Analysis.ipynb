{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df30d93",
   "metadata": {},
   "source": [
    "## Yelp reviews sentiment analysis using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e173c4",
   "metadata": {},
   "source": [
    "The below code is focused on exploring various aspects of Natural Language Processing (NLP) and building classifiers for sentiment analysis using PyTorch. Here's a breakdown of what the code does:\n",
    "\n",
    "1. **Data Processing**: The code processes raw Yelp review data, converts it to a pandas DataFrame, and manipulates the data as needed.\n",
    "\n",
    "2. **Data Vectorization and Vocabulary**: It creates a vocabulary for the NLP task and vectorizes the text reviews using one-hot encoding. A Vocabulary class is implemented to handle the mapping between tokens and indices.\n",
    "\n",
    "3. **Data Processing in PyTorch**: The code uses PyTorch's DataLoader to batch, shuffle, and load the data in parallel using multiprocessing workers.\n",
    "\n",
    "4. **Models Implementation**:\n",
    "   - A simple perceptron (`ReviewClassifier`) with a single linear layer and sigmoid activation for binary classification.\n",
    "   - A multi-layer perceptron (`MultiLayerPerceptron`) with hidden layers, ReLU activation, and dropout for regularization.\n",
    "   - A Convolutional Neural Network (`ConvolutionNeuralNetwork`) using 1D convolutions for sequence-based analysis.\n",
    "\n",
    "5. **Training and Evaluation**: The code includes training and evaluation loops for each epoch. It tracks train and validation loss, accuracy, and evaluates the model's performance on a test dataset.\n",
    "\n",
    "6. **Inference and Prediction**: It demonstrates how to predict the sentiment rating of new reviews using the trained model and the vectorizer.\n",
    "\n",
    "7. **Weight Inspection**: The code inspects the influential words in positive and negative reviews by analyzing the weights of the models first fully connected layer (`fcl1`).\n",
    "\n",
    "8. **Hyperparameters Tuning and Tracking**: The code uses various hyperparameters and keeps track of training and validation loss, as well as accuracy.\n",
    "\n",
    "9. **Device Usage and CUDA**: The code checks for the availability of CUDA and uses GPU acceleration if available.\n",
    "\n",
    "Overall, it provides a comprehensive exploration of building and training different types of neural network models for sentiment analysis tasks, ranging from simple perceptrons to more complex models like multi-layer perceptrons and convolutional neural networks. It also covers preprocessing steps, vocabulary creation, and evaluation of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474b8be",
   "metadata": {},
   "source": [
    "### Data Processing: \n",
    "1. import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edc8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8acb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument definations \n",
    "arg = Namespace(\n",
    "    islite = False,\n",
    "    train_csv_lite_with_split = \"data/yelp/reviews_with_splits_lite.csv\",\n",
    "    test_csv = \"data/yelp/raw_test.csv\",\n",
    "    train_csv = \"data/yelp/raw_train.csv\",   \n",
    "    test_split_ratio = 0.25,\n",
    "    train_split_ratio = 0.75,\n",
    "    seed = 1330,\n",
    "    output_file_sentiment = \"data/yelp/final_sentiment_list.csv\",\n",
    "    output_file_rating = \"data/yelp/final_ratings_list.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415dd5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading of data and removing the null reviews\n",
    "if not arg.islite:\n",
    "  test_csv = pd.read_csv(arg.test_csv, header=None, names=[\"rating\",\"review\"])\n",
    "  train_csv  = pd.read_csv(arg.train_csv, header=None, names = [\"rating\",\"review\"])\n",
    "  test_csv = test_csv[~pd.isnull(test_csv.review)]\n",
    "  train_csv = train_csv[~pd.isnull(train_csv.review)]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e877bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Contrary to other reviews, I have zero complai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Last summer I had an appointment to get new ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Friendly staff, same starbucks fair you get an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The food is good. Unfortunately the service is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Even when we didn't have a car Filene's Baseme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Picture Billy Joel's \\\"Piano Man\\\" DOUBLED mix...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0       2  Contrary to other reviews, I have zero complai...\n",
       "1       1  Last summer I had an appointment to get new ti...\n",
       "2       2  Friendly staff, same starbucks fair you get an...\n",
       "3       1  The food is good. Unfortunately the service is...\n",
       "4       2  Even when we didn't have a car Filene's Baseme...\n",
       "5       2  Picture Billy Joel's \\\"Piano Man\\\" DOUBLED mix..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the test\n",
    "test_csv.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47d794",
   "metadata": {},
   "source": [
    "2. Partitions the training data into train and validation sets based on ratings, shuffles within rating groups, and combines with test data.\n",
    "   Process the review text.\n",
    "   Change the numerical ratings to sentiments (1 for negative and 2 for positive).\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a36dd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "  if type(text) == float:\n",
    "        print(text)\n",
    "  text = text.lower()\n",
    "  text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "  text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "  return text\n",
    "\n",
    "def partition_dataset(train_csv, test_csv):\n",
    "  \n",
    "  rating_dict = collections.defaultdict(list)\n",
    "  for _, row in train_csv.iterrows():\n",
    "    rating_dict[row.rating].append(row.to_dict())\n",
    "  final_list =[]\n",
    "  np.random.seed(arg.seed)\n",
    "  for _,  item_list in sorted(rating_dict.items()):\n",
    "    np.random.shuffle(item_list)\n",
    "    total_rows = len(item_list)\n",
    "    total_train_required = int(arg.train_split_ratio*total_rows)\n",
    "    total_test_required = int(arg.test_split_ratio*total_rows)\n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:total_train_required]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[total_train_required:total_train_required+total_test_required]:\n",
    "        item['split'] = 'val'\n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)\n",
    "    for _, row in test_csv.iterrows():\n",
    "      row_dict = row.to_dict()\n",
    "      row_dict['split'] = 'test'\n",
    "      final_list.append(row_dict)\n",
    "\n",
    "    return final_list, pd.DataFrame(final_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5c6d8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    210000\n",
       "val       70000\n",
       "test      38000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list, final_list_df = partition_dataset(train_csv,test_csv)\n",
    "final_list_df.review = final_list_df.review.apply(preprocess_data)\n",
    "final_list_rating = final_list_df.copy()\n",
    "final_list_df.split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03ecedc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>used to come here a lot then i think the owner...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>got a notice for the preferred customer sale l...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>the burgers here are probably the best burgers...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i am a road warrior . i am used to eating alon...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>i come to this place whenever i am in town for...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  used to come here a lot then i think the owner...  train\n",
       "1  negative  got a notice for the preferred customer sale l...  train\n",
       "2  negative  the burgers here are probably the best burgers...  train\n",
       "3  negative  i am a road warrior . i am used to eating alon...  train\n",
       "4  negative  i come to this place whenever i am in town for...  train"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_sentiment = final_list_df.copy()\n",
    "final_list_sentiment['rating'] = final_list_sentiment.rating.apply({1: 'negative', 2: 'positive'}.get)\n",
    "final_list_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ea9a247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>used to come here a lot then i think the owner...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>got a notice for the preferred customer sale l...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the burgers here are probably the best burgers...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i am a road warrior . i am used to eating alon...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>i come to this place whenever i am in town for...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review  split\n",
       "0       1  used to come here a lot then i think the owner...  train\n",
       "1       1  got a notice for the preferred customer sale l...  train\n",
       "2       1  the burgers here are probably the best burgers...  train\n",
       "3       1  i am a road warrior . i am used to eating alon...  train\n",
       "4       1  i come to this place whenever i am in town for...  train"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aff6c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_rating.to_csv(arg.output_file_rating)\n",
    "final_list_sentiment.to_csv(arg.output_file_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16048f",
   "metadata": {},
   "source": [
    "### Data Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1339f9",
   "metadata": {},
   "source": [
    "####  Data Vocabulary\n",
    "Vocabulary class is commonly used in NLP tasks to convert words or tokens into numerical indices, which can then be used as input features for machine learning models. It provides methods for token addition, lookup, serialization, and deserialization, making it a fundamental component in many NLP pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e92b6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    A class for processing text and building a vocabulary for mapping tokens to indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token='<UNK>'):\n",
    "        \"\"\"\n",
    "        Initialize the Vocabulary object.\n",
    "        \n",
    "        Args:\n",
    "            token_to_idx (dict): Mapping of tokens to indices.\n",
    "            add_unk (bool): Whether to add an unknown token.\n",
    "            unk_token (str): Token to represent unknown words.\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary that can be serialized.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Serializable representation of the Vocabulary object.\n",
    "        \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Instantiate a Vocabulary object from a serialized dictionary.\n",
    "        \n",
    "        Args:\n",
    "            contents (dict): Serialized representation of the Vocabulary object.\n",
    "        \n",
    "        Returns:\n",
    "            Vocabulary: Instantiated Vocabulary object.\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Update the mapping dictionary based on the token and return its index.\n",
    "        \n",
    "        Args:\n",
    "            token (str): Token to be added to the vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            int: Index of the added token.\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "\n",
    "        return index\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        Retrieve the index of a token from the vocabulary.\n",
    "        \n",
    "        Args:\n",
    "            token (str): Token to look up.\n",
    "        \n",
    "        Returns:\n",
    "            int: Index of the token in the vocabulary.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            if token not in self._token_to_idx:\n",
    "                return 0\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve the token based on the index from the mapping dictionary.\n",
    "        \n",
    "        Args:\n",
    "            index (int): Index to look up.\n",
    "        \n",
    "        Returns:\n",
    "            str: Token corresponding to the index.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"The provided index: %d is not in the vocab\" % index)\n",
    "        else:\n",
    "            return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the Vocabulary object.\n",
    "        \n",
    "        Returns:\n",
    "            str: String representation of the Vocabulary object.\n",
    "        \"\"\"\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the vocabulary.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of unique tokens in the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self._token_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1235dd1",
   "metadata": {},
   "source": [
    "ReviewVectorizer class is used to convert text reviews into numerical vectors using the provided vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "53dd9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\"\n",
    "    Used the Vocabulary class to convert the tokens into actual numerical vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        \"\"\"\n",
    "        Initialize the ReviewVectorizer.\n",
    "\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): Maps word to integer.\n",
    "            rating_vocab (Vocabulary): Maps class label to integer (\"negative/positive\").\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\"\n",
    "        Vectorize a text review to one-hot encoding.\n",
    "\n",
    "        Args:\n",
    "            review (str): Text review.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: One-hot encoding vector.\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        \"\"\"\n",
    "        Instantiate a vectorizer for reviews directly from a dataset DataFrame.\n",
    "\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): Dataset containing reviews and ratings.\n",
    "            cutoff (int): Count threshold for word inclusion.\n",
    "\n",
    "        Returns:\n",
    "            ReviewVectorizer: Instantiated ReviewVectorizer.\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for rating in sorted(set(review_df.rating)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        word_count = collections.Counter()\n",
    "\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_count[word] += 1\n",
    "\n",
    "        for word, count in word_count.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Instantiate a vectorizer from serialized contents.\n",
    "\n",
    "        Args:\n",
    "            contents (dict): Serialized representation of the vectorizer.\n",
    "\n",
    "        Returns:\n",
    "            ReviewVectorizer: Instantiated ReviewVectorizer.\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Convert the vectorizer to a serializable representation.\n",
    "\n",
    "        Returns:\n",
    "            dict: Serializable representation of the vectorizer.\n",
    "        \"\"\"\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df321ea",
   "metadata": {},
   "source": [
    "### Data Processing in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbbe3b",
   "metadata": {},
   "source": [
    "ReviewDataset class is designed for handling text classification tasks using PyTorch. The dataset contains reviews and their corresponding ratings, and the purpose of this class is to facilitate the loading, processing, and manipulation of the data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "77478b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Initialize the ReviewDataset.\n",
    "\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): Dataset containing reviews and splits\n",
    "            vectorizer (ReviewVectorizer): Vectorizer for converting reviews to vectors\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # Split the dataset into train, validation, and test subsets\n",
    "        self.train_df = self.review_df[self.review_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.review_df[self.review_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_df = self.review_df[self.review_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        # Lookup dictionary for splits\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"\n",
    "        Load dataset and create a new vectorizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            review_csv (str): Location of the dataset CSV file\n",
    "\n",
    "        Returns:\n",
    "            ReviewDataset: An instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        train_review_df = review_df[review_df.split == 'train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        Load dataset and a cached vectorizer.\n",
    "\n",
    "        Args:\n",
    "            review_csv (str): Location of the dataset CSV file\n",
    "            vectorizer_filepath (str): Location of the saved vectorizer file\n",
    "\n",
    "        Returns:\n",
    "            ReviewDataset: An instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        Load a vectorizer from a file.\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): Location of the serialized vectorizer file\n",
    "\n",
    "        Returns:\n",
    "            ReviewVectorizer: An instance of ReviewVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        Save the vectorizer to a file using JSON.\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): Location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Get the associated vectorizer.\n",
    "\n",
    "        Returns:\n",
    "            ReviewVectorizer: The vectorizer used by the dataset\n",
    "        \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Set the active split of the dataset.\n",
    "\n",
    "        Args:\n",
    "            split (str): One of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the active split.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of data points in the active split\n",
    "        \"\"\"\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a data point from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the data point\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = self._vectorizer.vectorize(row.review)\n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector, 'y_target': rating_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Calculate the number of batches in the dataset.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Size of each batch\n",
    "\n",
    "        Returns:\n",
    "            int: Number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815e991",
   "metadata": {},
   "source": [
    "The function generates batches of data from a PyTorch dataset using a DataLoader, ensuring tensors are moved to a specified device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "acce1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generates batches of data from a PyTorch dataset using DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): The dataset to generate batches from.\n",
    "        batch_size (int): The size of each batch.\n",
    "        shuffle (bool, optional): Whether to shuffle the dataset. Default is True.\n",
    "        drop_last (bool, optional): Whether to drop the last incomplete batch if its size is less than batch_size.\n",
    "                                   Default is True.\n",
    "        device (str, optional): The device to move the tensors to, e.g., \"cuda\" or \"cpu\". Default is \"cpu\".\n",
    "    \n",
    "    Yields:\n",
    "        dict: A dictionary containing batched tensors moved to the specified device.\n",
    "    \"\"\"\n",
    "    # Create a DataLoader instance\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    # Iterate through the batches\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        # Move each tensor to the specified device\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bbde0",
   "metadata": {},
   "source": [
    "### Deep Learning Models: Single Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83758fb3",
   "metadata": {},
   "source": [
    "A basic perceptron comprises a sole linear layer that performs an affine transformation, followed by passing the output through an activation function. In this case, we are employing the sigmoid activation function, which introduces non-linearity to the model's output. This process enables the perceptron to capture complex relationships within the data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b82afeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple perceptron classifier using PyTorch's neural network module (nn.Module).\n",
    "    \n",
    "    Args:\n",
    "        num_features (int): Number of input features for the linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fcl = nn.Linear(in_features=num_features, out_features=1)\n",
    "  \n",
    "    def forward(self, vectorized_review, apply_sigmoid=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the perceptron classifier.\n",
    "        \n",
    "        Args:\n",
    "            vectorized_review (torch.Tensor): Input data tensor after vectorization.\n",
    "            apply_sigmoid (bool): Whether to apply sigmoid activation to the output.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after the linear transformation and activation (if applied).\n",
    "        \"\"\"\n",
    "        output = self.fcl(vectorized_review).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            output = torch.sigmoid(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa14fe",
   "metadata": {},
   "source": [
    "### Deep Learning Models:Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6c70e",
   "metadata": {},
   "source": [
    "A multilayer perceptron (MLP) comprises interconnected layers of neurons, with optional softmax output. Here, dropout regularization is applied to prevent overfitting. Dropout prevents coadaptation between neurons, mitigating overfitting by randomly deactivating connections during training. This promotes network robustness and avoids over-reliance on specific connections. Dropout introduces randomness without adding parameters, controlled by a \"drop probability\" hyperparameter (often 0.5). The provided MLP with dropout showcases its practical implementation for building resilient neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "da5ab71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple multi-layer perceptron model.\n",
    "\n",
    "    Args:\n",
    "        in_dimension (int): Input dimension size.\n",
    "        hidden_dimension (int): Hidden layer dimension size.\n",
    "        out_dimension (int): Output dimension size.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dimension, hidden_dimension, out_dimension):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fcl1 = nn.Linear(in_dimension, hidden_dimension)\n",
    "        self.fcl2 = nn.Linear(hidden_dimension, out_dimension)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        The forward pass for the multi-layer perceptron.\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): Input tensor.\n",
    "            apply_softmax (bool): Whether to apply softmax to the output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        # Pass through the first linear layer\n",
    "        first_layer_output = self.fcl1(x_in)\n",
    "\n",
    "        # Apply ReLU activation to the intermediate result\n",
    "        intermediate = F.relu(first_layer_output)\n",
    "\n",
    "        # Apply dropout to the intermediate result and pass through the second linear layer\n",
    "        output = self.fcl2(F.dropout(intermediate, p=0.5))\n",
    "\n",
    "        if apply_softmax:\n",
    "            # Apply softmax if specified\n",
    "            output = F.softmax(output, dim=1)\n",
    "\n",
    "        # Squeeze the output tensor\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c9c09",
   "metadata": {},
   "source": [
    "### Convolutions neural networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c656c",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs):\n",
    "\n",
    "CNNs involve convolving an input matrix with a kernel matrix through element-wise multiplication and summation. This produces an output matrix containing crucial information. The significance can be understood based on CNN dimensionality:\n",
    "\n",
    "**Dimensionality:** CNNs come in 1D, 2D, and 3D (Conv1d, Conv2d, and Conv3d). 1D is ideal for time series, like text. 2D serves image processing with width and height. 3D suits videos, adding the time dimension.\n",
    "\n",
    "**Channels:** These are input dimensions. For 2D convolutions, like images with color channels, there are 3 channels. Analogously, text uses a vocabulary length. Parameters: in_channel, out_channel.\n",
    "\n",
    "**Kernel Size:** The size of the kernel matrix, akin to n-grams in NLP. Larger size captures more context.\n",
    "\n",
    "**Stride:** Controls convolution steps. If stride equals kernel size, no overlap occurs, yielding smaller output. Smaller stride yields larger output.\n",
    "\n",
    "**Padding:** Adds 0s to input edges, aligning with stride during convolution.\n",
    "\n",
    "**Dilation:** Adjusts kernel application by introducing gaps.\n",
    "\n",
    "CNNs aim to configure convolution layers for desired features. Initial layers extract features, followed by processing. In classification, Linear (fc) layers handle classification. Implementation involves designing a feature vector. An artificial data tensor mirrors real data in shape. Data tensor is 3D for minibatch, where sequences of one-hot vectors become matrices, constituting the input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c6cf0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvolutionNeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, num_channels, output_size):\n",
    "        super(ConvolutionNeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=num_channels, kernel_size=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=1, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=1, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # Define the fully connected layer\n",
    "        self.fcl = nn.Linear(num_channels, output_size)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # Pass the input through the convolutional layers\n",
    "        features = self.convnet(x_in)\n",
    "        \n",
    "        # Pass the features through the fully connected layer\n",
    "        prediction_vector = self.fcl(features)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            # Apply softmax if required\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=2)\n",
    "\n",
    "        return prediction_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bedda8b",
   "metadata": {},
   "source": [
    "### Training and Validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "75a0b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='data/yelp/',\n",
    "    vectorizer_file='vectorizer.json',\n",
    "    batch_size=256,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=5,\n",
    "    seed=1330,\n",
    "    cuda = True,\n",
    "    device = torch.device(\"cuda\"))\n",
    "\n",
    "\n",
    "def make_train_state(args):\n",
    "  return {'epoch_index':0,\n",
    "          'train_loss':[],\n",
    "          'train_acc':[],\n",
    "          'val_loss':[],\n",
    "          'val_acc':[],\n",
    "          'test_loss':-1,\n",
    "          'test_acc':1\n",
    "  }\n",
    "\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f20dcb",
   "metadata": {},
   "source": [
    "CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA. It enables efficient computation on NVIDIA GPUs, accelerating various tasks by offloading them to the GPU for faster processing compared to traditional CPU-based computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63cf1269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "# Check the availabily of the CUDA\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "577c65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    args.device = torch.device(\"cpu\")\n",
    "    \n",
    "import json\n",
    "# Load dataset and create vectorizer\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "# Get the vectorizer from the dataset\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Initialize the classifier (Choose one of the following)\n",
    "# 1. For Single Layer Perceptron\n",
    "# classifier = SingleLayerPerceptron(in_dimension=len(vectorizer.review_vocab), out_dimension=1)\n",
    "# 2. For MultiLayer Perceptron\n",
    "classifier = MultiLayerPerceptron(in_dimension=len(vectorizer.review_vocab), hidden_dimension=100, out_dimension=1)\n",
    "# 3. For Convolutional Neural Network\n",
    "# classifier = ConvolutionNeuralNetwork(in_channels=len(vectorizer.review_vocab), num_channels=256, output_size=1)\n",
    "\n",
    "# Move the classifier to the selected device (CUDA or CPU)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# Function to compute accuracy\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred) > 0.5).cpu().long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a52e67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.2103, Train Accuracy: 92.0398\n",
      "Epoch: 0, Val Loss: 0.2186, Val Accuracy: 91.8742\n",
      "Epoch: 1, Train Loss: 0.2074, Train Accuracy: 92.1529\n",
      "Epoch: 1, Val Loss: 0.2167, Val Accuracy: 91.9329\n",
      "Epoch: 2, Train Loss: 0.2008, Train Accuracy: 92.4074\n",
      "Epoch: 2, Val Loss: 0.2156, Val Accuracy: 91.9388\n",
      "Epoch: 3, Train Loss: 0.1930, Train Accuracy: 92.7007\n",
      "Epoch: 3, Val Loss: 0.2153, Val Accuracy: 91.9189\n",
      "Epoch: 4, Train Loss: 0.1850, Train Accuracy: 93.0078\n",
      "Epoch: 4, Val Loss: 0.2165, Val Accuracy: 91.8713\n"
     ]
    }
   ],
   "source": [
    "# Method to train and evaluate\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    \n",
    "    # Set the dataset split to 'train'\n",
    "    dataset.set_split('train')\n",
    "    \n",
    "    # Generate batches for training\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    # Set the classifier in training mode\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Extract input data\n",
    "        x_in = batch_dict['x_data']\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = classifier(x_in.float())\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    # Update training state\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    # Calculate average training loss and accuracy\n",
    "    average_train_loss = np.mean(train_state['train_loss'])\n",
    "    average_train_acc = np.mean(train_state['train_acc'])\n",
    "    \n",
    "    # Print training results\n",
    "    print(\"Epoch: {}, Train Loss: {:.4f}, Train Accuracy: {:.4f}\".format(epoch_index, average_train_loss, average_train_acc))\n",
    "\n",
    "    # Validation loop\n",
    "    # Set the dataset split to 'val'\n",
    "    dataset.set_split('val')\n",
    "    \n",
    "    # Generate batches for validation\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    # Set the classifier in evaluation mode\n",
    "    classifier.eval()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # Forward pass\n",
    "        y_pred = classifier(batch_dict['x_data'].float())\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    # Update training state\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    average_val_loss = np.mean(train_state['val_loss'])\n",
    "    average_val_acc = np.mean(train_state['val_acc'])\n",
    "    \n",
    "    # Print validation results\n",
    "    print(\"Epoch: {}, Val Loss: {:.4f}, Val Accuracy: {:.4f}\".format(epoch_index, average_val_loss, average_val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e11886",
   "metadata": {},
   "source": [
    "### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5134cfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Test Loss: 0.23524149926379326, and Test Accuracy: 91.56494140624997\n"
     ]
    }
   ],
   "source": [
    "# Testing accuracy\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "\n",
    "# Initialize variables to track loss and accuracy\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# Set the classifier to evaluation mode\n",
    "classifier.eval()\n",
    "\n",
    "print(\"Testing\")\n",
    "\n",
    "# Iterate through batches in the test dataset\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # Get predictions from the classifier\n",
    "    y_pred = classifier(batch_dict['x_data'].float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "    loss_batch = loss.item()\n",
    "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "# Store the test loss and accuracy in the train_state dictionary\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "\n",
    "# Calculate average test loss and accuracy\n",
    "average_test_loss = train_state['test_loss']\n",
    "average_test_acc = train_state['test_acc']\n",
    "\n",
    "# Print the results\n",
    "print(\"Test Loss: {}, and Test Accuracy: {}\".format(average_test_loss, average_test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a4bd2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict the rating of a review using a trained classifier and vectorizer.\n",
    "\n",
    "    Args:\n",
    "        review (str): The input review text.\n",
    "        classifier (nn.Module): The trained classifier model.\n",
    "        vectorizer (ReviewVectorizer): The vectorizer used for encoding reviews.\n",
    "        decision_threshold (float, optional): The decision threshold for classification.\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted rating ('positive' or 'negative') for the review.\n",
    "    \"\"\"\n",
    "    # Preprocess the review text\n",
    "    review = preprocess_data(review)\n",
    "\n",
    "    # Vectorize the preprocessed review\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review=review)).to(args.device)\n",
    "\n",
    "    # Pass the vectorized review through the classifier\n",
    "    result = classifier(vectorized_review.view(1, -1))\n",
    "\n",
    "    # Calculate the probability using sigmoid activation\n",
    "    probability = torch.sigmoid(result).item()\n",
    "\n",
    "    # Determine the predicted rating based on the decision threshold\n",
    "    index = 1  # Default to positive rating\n",
    "    if probability < decision_threshold:\n",
    "        index = 0  # Predict negative rating\n",
    "\n",
    "    # Look up the rating label using the index\n",
    "    predicted_rating = vectorizer.rating_vocab.lookup_index(index)\n",
    "\n",
    "    return predicted_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0fbfe817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 'Will come here again' --> Predicted Rating: positive\n",
      "Review: 'if i could give . . . i would . don t do it .' --> Predicted Rating: negative\n",
      "Review: 'great store with good discounts on quality products .' --> Predicted Rating: positive\n",
      "Review: 'the only thing keeping me from raging on the employees right now is the insert of bread they give you with your sandwich . enjoy over paying for a worse version of jimmy johns . dont come here . ' --> Predicted Rating: negative\n"
     ]
    }
   ],
   "source": [
    "new_review = \"Will come here again\"\n",
    "prediction = predict_rating(new_review, classifier, vectorizer)\n",
    "print(f\"Review: '{new_review}' --> Predicted Rating: {prediction}\")\n",
    "\n",
    "new_review = \"if i could give . . . i would . don t do it .\"\n",
    "prediction = predict_rating(new_review, classifier, vectorizer)\n",
    "print(f\"Review: '{new_review}' --> Predicted Rating: {prediction}\")\n",
    "\n",
    "new_review = \"great store with good discounts on quality products .\"\n",
    "prediction = predict_rating(new_review, classifier, vectorizer)\n",
    "print(f\"Review: '{new_review}' --> Predicted Rating: {prediction}\")\n",
    "\n",
    "new_review = \"the only thing keeping me from raging on the employees right now is the insert of bread they give you with your sandwich . enjoy over paying for a worse version of jimmy johns . dont come here . \"\n",
    "prediction = predict_rating(new_review, classifier, vectorizer)\n",
    "print(f\"Review: '{new_review}' --> Predicted Rating: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c66d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top influential words in Positive Reviews:\n",
      "pleasantly\n",
      "guide\n",
      "deliciousness\n",
      "phenomenal\n",
      "nthank\n",
      "solid\n",
      "delicious\n",
      "fabulous\n",
      "drawback\n",
      "excellent\n",
      "cozy\n",
      "scrumptious\n",
      "yum\n",
      "web\n",
      "fear\n",
      "ngreat\n",
      "mongolian\n",
      "consistent\n",
      "mmmm\n",
      "vegas\n",
      "\n",
      "Top influential words in Negative Reviews:\n",
      "worst\n",
      "meh\n",
      "mediocre\n",
      "bland\n",
      "terrible\n",
      "unfriendly\n",
      "blah\n",
      "unfortunately\n",
      "slowest\n",
      "downhill\n",
      "inedible\n",
      "lacked\n",
      "inconsistent\n",
      "shame\n",
      "horrible\n",
      "rude\n",
      "dried\n",
      "towels\n",
      "soggy\n",
      "disgusting\n"
     ]
    }
   ],
   "source": [
    "# Weight inspection to identify most influential positive and negative words\n",
    "fcl_weights = classifier.fcl1.weight.detach()[0]\n",
    "_, indices = torch.sort(fcl_weights, dim=0, descending=True)\n",
    "indices = indices.cpu()\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "print(\"Top influential words in Positive Reviews:\")\n",
    "for i in range(20):\n",
    "    word = vectorizer.review_vocab.lookup_index(indices[i])\n",
    "    print(word)\n",
    "print(\"\")    \n",
    "print(\"Top influential words in Negative Reviews:\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    word = vectorizer.review_vocab.lookup_index(indices[i])\n",
    "    print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
